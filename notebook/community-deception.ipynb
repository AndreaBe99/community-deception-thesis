{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Community Deception"
      ],
      "metadata": {
        "id": "p0PM-10o4qNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZInwz0n74vdY",
        "outputId": "86faa84f-d561-40bf-c077-2a1177398ada"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Pytorch Geometric"
      ],
      "metadata": {
        "id": "fMmiVKrL4qNt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we are on Kaggle we need to run the following cells to install Pytorch Geometric"
      ],
      "metadata": {
        "id": "TuaAoPQU4qNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "print(torch.__version__)\n",
        "os.environ[\"TORCH\"] = torch.__version__\n",
        "\n",
        "print(torch.version.cuda)\n",
        "os.environ[\"CUDA\"] = \"cu118\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-31T09:06:32.011475Z",
          "iopub.execute_input": "2023-08-31T09:06:32.012633Z",
          "iopub.status.idle": "2023-08-31T09:06:36.050190Z",
          "shell.execute_reply.started": "2023-08-31T09:06:32.012592Z",
          "shell.execute_reply": "2023-08-31T09:06:36.048323Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhb60JfQ4qNu",
        "outputId": "f1d26d90-189b-4aec-bde2-64d762866c38"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.1+cu118\n",
            "11.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torch_geometric\n",
        "# ! pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-${TORCH}+${CUDA}.html\n",
        "! pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-${TORCH}.html"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xrzyr2i4qNw",
        "outputId": "66e9ddf7-66a4-4967-a89e-8b4e1019d7aa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2023.7.22)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.2.0)\n",
            "Building wheels for collected packages: torch_geometric\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910454 sha256=f5c74fa48dd16e8e3020e1c1a492a2ff3e160c0d2dfc2f54dfb30e8fd215a69b\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n",
            "Successfully built torch_geometric\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.3.1\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
            "Collecting pyg_lib\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/pyg_lib-0.2.0%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_scatter-2.1.1%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_sparse-0.6.17%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_cluster-1.6.1%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_spline_conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_spline_conv-1.2.2%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (884 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m884.9/884.9 kB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_sparse) (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy->torch_sparse) (1.23.5)\n",
            "Installing collected packages: torch_spline_conv, torch_scatter, pyg_lib, torch_sparse, torch_cluster\n",
            "Successfully installed pyg_lib-0.2.0+pt20cu118 torch_cluster-1.6.1+pt20cu118 torch_scatter-2.1.1+pt20cu118 torch_sparse-0.6.17+pt20cu118 torch_spline_conv-1.2.2+pt20cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install igraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0SJtojz557z",
        "outputId": "cfa1195f-1502-49a8-c305-dd13174b3afc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting igraph\n",
            "  Downloading igraph-0.10.6-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting texttable>=1.6.2 (from igraph)\n",
            "  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n",
            "Installing collected packages: texttable, igraph\n",
            "Successfully installed igraph-0.10.6 texttable-1.6.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "z5ImUEyx4qNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import torch and os another time to reset the colab enviroment after PyG installation\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Typing\n",
        "from typing import List, Tuple\n",
        "from collections import Counter\n",
        "\n",
        "# Deep Learning\n",
        "from torch_geometric.utils import from_networkx\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.data import Batch\n",
        "from torch_geometric.nn import GCNConv, GATConv\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "from torch.distributions import MultivariateNormal\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "# Graph\n",
        "import networkx as nx\n",
        "import igraph as ig\n",
        "\n",
        "# Misc\n",
        "from enum import Enum\n",
        "import math\n",
        "import random\n",
        "import json\n",
        "\n",
        "# Plot\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('default')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-31T08:58:37.679350Z",
          "iopub.execute_input": "2023-08-31T08:58:37.679758Z",
          "iopub.status.idle": "2023-08-31T08:58:37.851120Z",
          "shell.execute_reply.started": "2023-08-31T08:58:37.679723Z",
          "shell.execute_reply": "2023-08-31T08:58:37.849489Z"
        },
        "trusted": true,
        "id": "39avPlWH4qNx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "5VGevrvC4qNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FilePaths(Enum):\n",
        "    \"\"\"Class to store file paths for data and models\"\"\"\n",
        "    # ° Local\n",
        "    # DATASETS_DIR = 'dataset/data'\n",
        "    # LOG_DIR    = 'src/logs/' # 'src/logs/'\n",
        "    # ° Kaggle\n",
        "    # DATASETS_DIR = '/kaggle/input/network-community'\n",
        "    # LOG_DIR    = 'logs/'\n",
        "    # ° Google Colab\n",
        "    DATASETS_DIR = \"/content/drive/MyDrive/Sapienza/Tesi/Datasets\"\n",
        "    LOG_DIR = \"/content/drive/MyDrive/Sapienza/Tesi/Logs/\"\n",
        "\n",
        "    # Dataset file paths\n",
        "    KAR = DATASETS_DIR + '/kar.mtx'\n",
        "    DOL = DATASETS_DIR + '/dol.mtx'\n",
        "    MAD = DATASETS_DIR + '/mad.mtx'\n",
        "    LESM = DATASETS_DIR + '/lesm.mtx'\n",
        "    POLB = DATASETS_DIR + '/polb.mtx'\n",
        "    WORDS = DATASETS_DIR + '/words.mtx'\n",
        "    ERDOS = DATASETS_DIR + '/erdos.mtx'\n",
        "    POW = DATASETS_DIR + '/pow.mtx'\n",
        "    FB_75 = DATASETS_DIR + '/fb-75.mtx'\n",
        "    DBLP = DATASETS_DIR + '/dblp.mtx'\n",
        "    ASTR = DATASETS_DIR + '/astr.mtx'\n",
        "    AMZ = DATASETS_DIR + '/amz.mtx'\n",
        "    YOU = DATASETS_DIR + '/you.mtx'\n",
        "    ORK = DATASETS_DIR + '/ork.mtx'\n",
        "\n",
        "\n",
        "class HyperParams(Enum):\n",
        "    \"\"\" Hyperparameters for the model.\"\"\"\n",
        "\n",
        "    \"\"\" Graph Encoder Parameters \"\"\"\"\"\n",
        "    G_IN_SIZE = 50\n",
        "    G_HIDDEN_SIZE = 50\n",
        "    G_EMBEDDING_SIZE = 50\n",
        "\n",
        "    \"\"\" Agent Parameters\"\"\"\n",
        "    HIDDEN_SIZE = 300\n",
        "    ACTION_STD = 0.5\n",
        "    EPS_CLIP = 0.2\n",
        "    LR = 0.0003\n",
        "    GAMMA = 0.99\n",
        "\n",
        "    \"\"\" Training Parameters \"\"\"\n",
        "    # Number of episodes to collect experience\n",
        "    MAX_EPISODES = 200          # 15000\n",
        "    # Maximum number of time steps per episode\n",
        "    MAX_TIMESTEPS = 10  # ! Unused, I set it to the double of the edge budget\n",
        "    # Update the policy after N timesteps\n",
        "    UPDATE_TIMESTEP = 100  # ! Unused, I set it to 10 times the edge budget\n",
        "    # Update policy for K epochs\n",
        "    K_EPOCHS = 20\n",
        "    # Print info about the model after N episodes\n",
        "    LOG_INTERVAL = 20\n",
        "    # Exit if the average reward is greater than this value\n",
        "    SOLVED_REWARD = 0.7\n",
        "    # Save model after N episodes\n",
        "    SAVE_MODEL = int(MAX_EPISODES / 10)\n",
        "    # Use a random seed\n",
        "    RANDOM_SEED = 42\n",
        "\n",
        "    \"\"\"Hyperparameters for the Environment\"\"\"\n",
        "    BETA = 30  # Numeber of possible action with BETA=30, is 30% of the edges\n",
        "    DEBUG = False\n",
        "    WEIGHT = 0.8\n",
        "\n",
        "\n",
        "class DetectionAlgorithms(Enum):\n",
        "    \"\"\"\n",
        "    Enum class for the detection algorithms\n",
        "    \"\"\"\n",
        "    LOUV = \"louvain\"\n",
        "    WALK = \"walktrap\"\n",
        "    GRE = \"greedy\"\n",
        "    INF = \"infomap\"\n",
        "    LAB = \"label_propagation\"\n",
        "    EIG = \"eigenvector\"\n",
        "    BTW = \"edge_betweenness\"\n",
        "    SPIN = \"spinglass\"\n",
        "    OPT = \"optimal\"\n",
        "    SCD = \"scalable_community_detection\"\n",
        "\n",
        "\n",
        "class Utils:\n",
        "    \"\"\"Class to store utility functions\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def get_device_placement():\n",
        "        \"\"\"Get device placement, CPU or GPU\"\"\"\n",
        "        return os.getenv(\"RELNET_DEVICE_PLACEMENT\", \"CPU\")\n",
        "\n",
        "    @staticmethod\n",
        "    def import_mtx_graph(file_path: str) -> nx.Graph:\n",
        "        \"\"\"\n",
        "        Import a graph from a .mtx file\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        file_path : str\n",
        "            File path of the .mtx file\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        nx.Graph\n",
        "            Graph imported from the .mtx file\n",
        "        \"\"\"\n",
        "        try:\n",
        "            graph_matrix = scipy.io.mmread(file_path)\n",
        "            graph = nx.Graph(graph_matrix)\n",
        "            for node in graph.nodes:\n",
        "                # graph.nodes[node]['name'] = node\n",
        "                graph.nodes[node]['num_neighbors'] = len(\n",
        "                    list(graph.neighbors(node)))\n",
        "            return graph\n",
        "        except Exception as exception:\n",
        "            print(\"Error: \", exception)\n",
        "            return None\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_avg_reward(\n",
        "            reward: List[float],\n",
        "            episode_length: List[int],\n",
        "            env_name: str,\n",
        "            file_path: str = FilePaths.LOG_DIR.value):\n",
        "        \"\"\"\n",
        "        Plot the average reward and the time steps of the episodes in the same\n",
        "        plot, using matplotlib, where the average reward is the blue line and\n",
        "        the episode length are the orange line.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        reward : List[float]\n",
        "            Average reward for each episode\n",
        "        episode_length : List[int]\n",
        "            Time steps for each episode\n",
        "        env_name : str\n",
        "            Environment name\n",
        "        file_path : str, optional\n",
        "            Path to save the plot, by default \"src/logs/\"\n",
        "        \"\"\"\n",
        "        fig, ax1 = plt.subplots()\n",
        "\n",
        "        color = 'tab:blue'\n",
        "        ax1.set_xlabel('Episode')\n",
        "        ax1.set_ylabel('Average Reward', color=color)\n",
        "        ax1.plot(reward, color=color)\n",
        "        ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "        ax2 = ax1.twinx()\n",
        "        color = 'tab:orange'\n",
        "        ax2.set_ylabel('Time Steps', color=color)\n",
        "        ax2.plot(episode_length, color=color)\n",
        "        ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "        fig.tight_layout()\n",
        "        plt.title(f'Average Reward and Time Steps per Episode ({env_name})')\n",
        "        file_name = f\"{file_path}{env_name}_avg_reward.png\"\n",
        "        plt.savefig(file_name, dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    @staticmethod\n",
        "    def write_results_to_json(\n",
        "            episodes_avg_reward: List[float],\n",
        "            episode_length: List[int],\n",
        "            hyperparameters: dict,\n",
        "            env_name: str,\n",
        "            file_path: str = FilePaths.LOG_DIR.value):\n",
        "        \"\"\"\n",
        "        Write the episodes_avg_reward, episode_length, and hyperparameters to a JSON file.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        episodes_avg_reward : List[float]\n",
        "            List of average rewards for each episode\n",
        "        episode_length : List[int]\n",
        "            List of episode lengths\n",
        "        hyperparameters : dict\n",
        "            Dictionary of hyperparameters used in the training process\n",
        "                file_path : str\n",
        "            Path to the output JSON file\n",
        "        \"\"\"\n",
        "        data = {\n",
        "            \"episodes_avg_reward\": episodes_avg_reward,\n",
        "            \"episode_length\": episode_length,\n",
        "            \"hyperparameters\": hyperparameters\n",
        "        }\n",
        "        file_name = f\"{file_path}{env_name}_results.json\"\n",
        "        with open(file_name, \"w\") as f:\n",
        "            json.dump(data, f, indent=4)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-08-31T09:10:58.218454Z",
          "iopub.execute_input": "2023-08-31T09:10:58.218794Z",
          "iopub.status.idle": "2023-08-31T09:10:58.696006Z",
          "shell.execute_reply.started": "2023-08-31T09:10:58.218746Z",
          "shell.execute_reply": "2023-08-31T09:10:58.694414Z"
        },
        "trusted": true,
        "id": "Pairxi9g4qNy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Community Algorithms"
      ],
      "metadata": {
        "id": "iJUWAWt24qNz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Communities Detection"
      ],
      "metadata": {
        "id": "ctdHdmia4qN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DetectionAlgorithm(object):\n",
        "    \"\"\"Class for the community detection algorithms\"\"\"\n",
        "\n",
        "    def __init__(self, alg_name: str) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the DetectionAlgorithm object\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        alg_name : str\n",
        "            The name of the algorithm\n",
        "        \"\"\"\n",
        "        self.alg_name = alg_name\n",
        "        self.ig_graph = None\n",
        "\n",
        "    def networkx_to_igraph(self, graph: nx.Graph) -> ig.Graph:\n",
        "        \"\"\"\n",
        "        Convert NetworkX graph to iGraph graph, in this way we can use\n",
        "        iGraph's community detection algorithms\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        graph : nx.Graph\n",
        "            The graph to be converted\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        ig.Graph\n",
        "            The converted graph\n",
        "        \"\"\"\n",
        "        self.ig_graph = ig.Graph.from_networkx(graph)\n",
        "        return self.ig_graph\n",
        "\n",
        "    def compute_community(self, graph: nx.Graph, args: dict = None) -> List[List[int]]:\n",
        "        \"\"\"\n",
        "        Compute the community detection algorithm\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        graph : nx.Graph\n",
        "            The graph to be computed\n",
        "        args : dict\n",
        "            The arguments for the algorithm\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        List[List[int]]\n",
        "            list of list of vertices in each cluster\n",
        "        \"\"\"\n",
        "        # Transform the graph to igraph\n",
        "        graph = self.networkx_to_igraph(graph)\n",
        "\n",
        "        # Rename DetectionAlgorithms Enum to da for convenience\n",
        "        da = DetectionAlgorithms\n",
        "        # Choose the algorithm\n",
        "        if self.alg_name == da.LOUV.value:\n",
        "            return self.compute_louv(graph, args)\n",
        "        elif self.alg_name == da.WALK.value:\n",
        "            return self.compute_walk(graph, args)\n",
        "        elif self.alg_name == da.GRE.value:\n",
        "            return self.compute_gre(graph, args)\n",
        "        elif self.alg_name == da.INF.value:\n",
        "            return self.compute_inf(graph, args)\n",
        "        elif self.alg_name == da.LAB.value:\n",
        "            return self.compute_lab(graph, args)\n",
        "        elif self.alg_name == da.EIG.value:\n",
        "            return self.compute_eig(graph, args)\n",
        "        elif self.alg_name == da.BTW.value:\n",
        "            return self.compute_btw(graph, args)\n",
        "        elif self.alg_name == da.SPIN.value:\n",
        "            return self.compute_spin(graph, args)\n",
        "        elif self.alg_name == da.OPT.value:\n",
        "            return self.compute_opt(graph, args)\n",
        "        elif self.alg_name == da.SCD.value:\n",
        "            return self.compute_scd(graph)\n",
        "        else:\n",
        "            raise ValueError('Invalid algorithm name')\n",
        "\n",
        "    def vertexcluster_to_list(self, cluster: ig.VertexClustering) -> List[List[int]]:\n",
        "        \"\"\"\n",
        "        Convert iGraph.VertexClustering object to list of list of vertices in each cluster\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        cluster : ig.VertexClustering\n",
        "            cluster from iGraph community detection algorithm\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        List[List[int]]\n",
        "            list of list of vertices in each cluster\n",
        "        \"\"\"\n",
        "        return [c for c in cluster]\n",
        "\n",
        "    def plot_graph(self) -> plt:\n",
        "        \"\"\"Plot the graph using iGraph\n",
        "\n",
        "        Returns\n",
        "        ---------\n",
        "        plot: plt\n",
        "            The plot of the graph\n",
        "\n",
        "        \"\"\"\n",
        "        # fig, ax = plt.subplots(figsize=(10, 10))\n",
        "        plot = ig.plot(\n",
        "            self.ig_graph,\n",
        "            mark_groups=True,\n",
        "            vertex_size=20,\n",
        "            edge_color='black',\n",
        "            vertex_label=[v.index for v in self.ig_graph.vs],\n",
        "            bbox=(0, 0, 500, 500),\n",
        "            # target=ax,\n",
        "        )\n",
        "        return plot\n",
        "\n",
        "    def compute_louv(self, graph: ig.Graph, args_louv: dict) -> List[List[int]]:\n",
        "        \"\"\"\n",
        "        Compute the Louvain community detection algorithm\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        graph : ig.Graph\n",
        "            The graph to be clustered\n",
        "        args_louv : dict\n",
        "            The arguments for the Louvain algorithm\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        List[List[int]]\n",
        "            list of list of vertices in each cluster\n",
        "        \"\"\"\n",
        "        if args_louv is None:\n",
        "            louv = graph.community_leiden()\n",
        "        else:\n",
        "            louv = graph.community_leiden(**args_louv)\n",
        "        return self.vertexcluster_to_list(louv)\n",
        "\n",
        "    def compute_walk(self, graph: ig.Graph, args_walk: dict) -> List[List[int]]:\n",
        "        \"\"\"\n",
        "        Compute the Walktrap community detection algorithm\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        graph : ig.Graph\n",
        "            The graph to be clustered\n",
        "        args_walk : dict\n",
        "            The arguments for the Walktrap algorithm\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        List[List[int]]\n",
        "            list of list of vertices in each cluster\n",
        "        \"\"\"\n",
        "        if args_walk is None:\n",
        "            walk = graph.community_walktrap()\n",
        "        else:\n",
        "            walk = graph.community_walktrap(**args_walk)\n",
        "        # Need to be converted to VertexClustering object\n",
        "        return self.vertexcluster_to_list(walk.as_clustering())\n",
        "\n",
        "    def compute_gre(self, graph: ig.Graph, args_gre: dict) -> List[List[int]]:\n",
        "        \"\"\"\n",
        "        Compute the Greedy community detection algorithm\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        graph : ig.Graph\n",
        "            The graph to be clustered\n",
        "        args_greed : dict\n",
        "            The arguments for the Greedy algorithm\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        List[List[int]]\n",
        "            list of list of vertices in each cluster\n",
        "        \"\"\"\n",
        "        if args_gre is None:\n",
        "            greed = graph.community_fastgreedy()\n",
        "        else:\n",
        "            greed = graph.community_fastgreedy(**args_gre)\n",
        "        # Need to be converted to VertexClustering object\n",
        "        return self.vertexcluster_to_list(greed.as_clustering())\n",
        "\n",
        "    def compute_inf(self, graph: ig.Graph, args_infomap: dict) -> List[List[int]]:\n",
        "        \"\"\"\n",
        "        Compute the Infomap community detection algorithm\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        graph : ig.Graph\n",
        "            The graph to be clustered\n",
        "        args_infomap : dict\n",
        "            The arguments for the Infomap algorithm\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        List[List[int]]\n",
        "            list of list of vertices in each cluster\n",
        "        \"\"\"\n",
        "        if args_infomap is None:\n",
        "            infomap = graph.community_infomap()\n",
        "        else:\n",
        "            infomap = graph.community_infomap(**args_infomap)\n",
        "        return self.vertexcluster_to_list(infomap)\n",
        "\n",
        "    def compute_lab(self, graph: ig.Graph, args_lab: dict) -> List[List[int]]:\n",
        "        \"\"\"\n",
        "        Compute the Label Propagation community detection algorithm\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        graph : ig.Graph\n",
        "            The graph to be clustered\n",
        "        args_lab : dict\n",
        "            The arguments for the Label Propagation algorithm\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        List[List[int]]\n",
        "            list of list of vertices in each cluster\n",
        "        \"\"\"\n",
        "        if args_lab is None:\n",
        "            lab = graph.community_label_propagation()\n",
        "        else:\n",
        "            lab = graph.community_label_propagation(**args_lab)\n",
        "        return self.vertexcluster_to_list(lab)\n",
        "\n",
        "    def compute_eig(self, graph: ig.Graph, args_eig: dict) -> List[List[int]]:\n",
        "        \"\"\"\n",
        "        Compute the Eigenvector community detection algorithm\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        graph : ig.Graph\n",
        "            The graph to be clustered\n",
        "        args_eig : dict\n",
        "            The arguments for the Eigenvector algorithm\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        List[List[int]]\n",
        "            list of list of vertices in each cluster\n",
        "        \"\"\"\n",
        "        if args_eig is None:\n",
        "            eig = graph.community_leading_eigenvector()\n",
        "        else:\n",
        "            eig = graph.community_leading_eigenvector(**args_eig)\n",
        "        return self.vertexcluster_to_list(eig)\n",
        "\n",
        "    def compute_btw(self, graph: ig.Graph, args_btw: dict) -> List[List[int]]:\n",
        "        \"\"\"\n",
        "        Compute the Edge Betweenness community detection algorithm\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        graph : ig.Graph\n",
        "            The graph to be clustered\n",
        "        args_btw : dict\n",
        "            The arguments for the Betweenness algorithm\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        List[List[int]]\n",
        "            list of list of vertices in each cluster\n",
        "        \"\"\"\n",
        "        if btw is None:\n",
        "            btw = graph.community_edge_betweenness()\n",
        "        else:\n",
        "            btw = graph.community_edge_betweenness(**args_btw)\n",
        "        # Need to be converted to VertexClustering object\n",
        "        return self.vertexcluster_to_list(btw.as_clustering())\n",
        "\n",
        "    def compute_spin(self, graph: ig.Graph, args_spin: dict) -> List[List[int]]:\n",
        "        \"\"\"\n",
        "        Compute the Spin Glass community detection algorithm\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        graph : ig.Graph\n",
        "            The graph to be clustered\n",
        "        args_spin : dict\n",
        "            The arguments for the Spin Glass algorithm\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        List[List[int]]\n",
        "            list of list of vertices in each cluster\n",
        "        \"\"\"\n",
        "        if args_spin is None:\n",
        "            spin = graph.community_spinglass()\n",
        "        else:\n",
        "            spin = graph.community_spinglass(**args_spin)\n",
        "        return self.vertexcluster_to_list(spin)\n",
        "\n",
        "    def compute_opt(self, graph: ig.Graph, args_opt: dict) -> List[List[int]]:\n",
        "        \"\"\"\n",
        "        Compute the Optimal community detection algorithm\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        graph : ig.Graph\n",
        "            The graph to be clustered\n",
        "        args_opt : dict\n",
        "            The arguments for the Optimal algorithm\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        List[List[int]]\n",
        "            list of list of vertices in each cluster\n",
        "        \"\"\"\n",
        "        if args_opt is None:\n",
        "            opt = graph.community_optimal_modularity()\n",
        "        else:\n",
        "            opt = graph.community_optimal_modularity(**args_opt)\n",
        "        return self.vertexcluster_to_list(opt)\n",
        "\n",
        "    def compute_scd(self, graph: ig.Graph) -> List[List[int]]:\n",
        "        \"\"\"\n",
        "        Compute the Surprise community detection algorithm\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        graph : ig.Graph\n",
        "            The graph to be clustered\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        List[List[int]]\n",
        "            list of list of vertices in each cluster\n",
        "        \"\"\"\n",
        "        # Write the graph to a text file\n",
        "        self.write_graph_to_file(graph, \"output.txt\")\n",
        "        # Execute SCD algorithm from the git submodule\n",
        "        os.system(\"./../src/SCD/build/scd -f output.txt\")\n",
        "        result_list = self.read_data_from_file('communities.dat')\n",
        "        return result_list\n",
        "\n",
        "    @staticmethod\n",
        "    def write_graph_to_file(graph: ig.Graph, file_path: str) -> None:\n",
        "        \"\"\"\n",
        "        Write the graph to a text file, where each line is an\n",
        "        edge in the graph.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        graph : ig.Graph\n",
        "            Graph object to write to file\n",
        "        file_path : str\n",
        "            file path of the output file\n",
        "        \"\"\"\n",
        "        with open(file_path, 'w', encoding='utf-8') as file:\n",
        "            for edge in graph.get_edgelist():\n",
        "                # To ensure we don't duplicate edges (x, y) and (y, x)\n",
        "                if edge[0] < edge[1]:\n",
        "                    file.write(f\"{edge[0]} {edge[1]}\\n\")\n",
        "\n",
        "    @staticmethod\n",
        "    def read_data_from_file(file_path: str) -> List[List[int]]:\n",
        "        \"\"\"\n",
        "        Read data from file and return a list of lists, where each row list of\n",
        "        nodes is a community.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        file_path : str\n",
        "            File path to the data file.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        List[List[int]]\n",
        "            List of lists, where each row list of nodes is a community.\n",
        "        \"\"\"\n",
        "        data_list = []\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            for line in file:\n",
        "                numbers = [int(num) for num in line.strip().split()]\n",
        "                data_list.append(numbers)\n",
        "        return data_list"
      ],
      "metadata": {
        "trusted": true,
        "id": "lXOSaaPJ4qN0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deception Score"
      ],
      "metadata": {
        "id": "NXaC3bxp4qN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DeceptionScore(object):\n",
        "    \"\"\"Deception score of a community detection algorithm.\"\"\"\n",
        "    def __init__(self, community_target: List[int]) -> None:\n",
        "        self.community_target = community_target\n",
        "\n",
        "    @staticmethod\n",
        "    def recall(g_i: List[int], community_target: List[int]) -> float:\n",
        "        \"\"\"Calculate recall score of a community g_i\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        g_i : List[int]\n",
        "            Community found by a community detection algorithm.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        float\n",
        "            Recall score of g_i.\n",
        "        \"\"\"\n",
        "        # Number of members in g_i that are also in our community\n",
        "        members_in_g_i = len(set(community_target) & set(g_i))\n",
        "        return members_in_g_i / len(community_target)\n",
        "\n",
        "    @staticmethod\n",
        "    def precision(g_i: List[int], community_target: List[int]) -> float:\n",
        "        \"\"\"Calculate precision score of a community g_i\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        g_i : List[int]\n",
        "            Community found by a community detection algorithm.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        float\n",
        "            Precision score of g_i.\n",
        "        \"\"\"\n",
        "        # Number of members in G_i that are also in our community\n",
        "        members_in_g_i = len(set(community_target) & set(g_i))\n",
        "        return members_in_g_i / len(g_i)\n",
        "\n",
        "    def compute_deception_score(\n",
        "            self,\n",
        "            community_structure: List[List[int]],\n",
        "            connected_components: int) -> float:\n",
        "        \"\"\"Calculate deception score of a community detection algorithm.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        community_structure : List(List(int))\n",
        "            Community structure found by a community detection algorithm.\n",
        "        connected_components : int\n",
        "            Number of connected components in the graph.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        deception_score : float\n",
        "            Deception score of a community detection algorithm.\n",
        "        \"\"\"\n",
        "        # Number of intersecting nodes between the community structure and community target\n",
        "        n_intersecting_nodes = [g_i for g_i in community_structure if len(\n",
        "            set(self.community_target) & set(g_i)) > 0]\n",
        "\n",
        "        recall = max([self.recall(g_i, self.community_target) for g_i in community_structure])\n",
        "        precision = sum([self.precision(g_i, self.community_target) for g_i in n_intersecting_nodes])\n",
        "\n",
        "        # Ideal situation occurs when each member of the community target is\n",
        "        # placed in a different community and the value of the maximum recall\n",
        "        # is lower possible.\n",
        "        community_spread = 1 - (connected_components - 1) / (len(self.community_target) - 1)\n",
        "\n",
        "        # Ideal situation occurs when each member of the community structure\n",
        "        # contains little percentage of the community target.\n",
        "        community_hiding = 0.5 * (1 - recall) + 0.5 * (1 - precision / len(n_intersecting_nodes))\n",
        "\n",
        "        # Deception score is the product of community spread and community hiding.\n",
        "        deception_score = community_spread * community_hiding\n",
        "        return deception_score"
      ],
      "metadata": {
        "trusted": true,
        "id": "ngVcvKf64qN1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normalized Mutual Information Score"
      ],
      "metadata": {
        "id": "Wg5dgtUo4qN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class NormalizedMutualInformation(object):\n",
        "    @staticmethod\n",
        "    def calculate_confusion_matrix(\n",
        "            communities_old: List[List[int]],\n",
        "            communities_new: List[List[int]]) -> Counter:\n",
        "        \"\"\"\n",
        "        Calculate the confusion matrix between two sets of communities.\n",
        "        Where the element (i, j) of the confusion matrix is the number of shared\n",
        "        members between an initially detected community C_i and the community\n",
        "        C_j after deception.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        communities_old : List[List[int]]\n",
        "            Communities before deception\n",
        "        communities_new : List[List[int]]\n",
        "            Communities after deception\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        confusion_matrix : Counter\n",
        "            Confusion matrix\n",
        "        \"\"\"\n",
        "        confusion_matrix = Counter()\n",
        "        #° Avoid to process the same community twice\n",
        "        #BUG ZeroDivisionError if we use this optimization\n",
        "        #BUG processed_new = set()\n",
        "        for i, old in enumerate(communities_old):\n",
        "            for j, new in enumerate(communities_new):\n",
        "                #BUG if j not in processed_new:\n",
        "                intersection = len(set(old) & set(new))\n",
        "                confusion_matrix[(i, j)] = intersection\n",
        "                #BUG    if intersection > 0:\n",
        "                #BUG        processed_new.add(j)\n",
        "        return confusion_matrix\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_sums(confusion_matrix: Counter) -> Tuple[Counter, Counter, int]:\n",
        "        \"\"\"\n",
        "        Calculate the row sums, column sums and total sum of a confusion matrix.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        confusion_matrix : Counter\n",
        "            Confusion matrix\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        (row_sums, col_sums, total_sum) : Tuple[Counter, Counter, int]\n",
        "            Tuple containing the row sums, column sums and total sum of the\n",
        "            confusion matrix.\n",
        "        \"\"\"\n",
        "        row_sums = Counter()\n",
        "        col_sums = Counter()\n",
        "        total_sum = 0\n",
        "        for (i, j), value in confusion_matrix.items():\n",
        "            row_sums[i] += value\n",
        "            col_sums[j] += value\n",
        "            total_sum += value\n",
        "        return row_sums, col_sums, total_sum\n",
        "\n",
        "    def compute_nmi(\n",
        "            self,\n",
        "            communities_old: List[List[int]],\n",
        "            communities_new: List[List[int]]) -> float:\n",
        "        \"\"\"\n",
        "        Calculate the normalized mutual information between two sets of\n",
        "        Communities.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        communities_old : List[List[int]]\n",
        "            List of communities before deception\n",
        "        communities_new : List[List[int]]\n",
        "            List of communities after deception\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        nmi : float\n",
        "            Normalized mutual information, value between 0 and 1.\n",
        "        \"\"\"\n",
        "        confusion_matrix = self.calculate_confusion_matrix(\n",
        "            communities_old, communities_new)\n",
        "        row_sums, col_sums, total_sum = self.calculate_sums(confusion_matrix)\n",
        "\n",
        "        # Numerator\n",
        "        nmi_numerator = 0\n",
        "        for (i, j), n_ij in confusion_matrix.items():\n",
        "            n_i = row_sums[i]\n",
        "            n_j = col_sums[j]\n",
        "            try:\n",
        "                nmi_numerator += n_ij * math.log((n_ij * total_sum) / (n_i * n_j))\n",
        "            except ValueError:\n",
        "                # We could get a math domain error if n_ij is 0\n",
        "                continue\n",
        "\n",
        "        # Denominator\n",
        "        nmi_denominator = 0\n",
        "        for i, n_i in row_sums.items():\n",
        "            nmi_denominator += n_i * math.log(n_i / total_sum)\n",
        "        for j, n_j in col_sums.items():\n",
        "            nmi_denominator += n_j * math.log(n_j / total_sum)\n",
        "        # Normalized mutual information\n",
        "        nmi_score = -2 * nmi_numerator / nmi_denominator\n",
        "        return nmi_score\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "-UVHWawt4qN2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enviroment"
      ],
      "metadata": {
        "id": "-l6FamFa4qN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class GraphEnvironment(object):\n",
        "    \"\"\"Enviroment where the agent will act, it will be a graph with a community\"\"\"\n",
        "\n",
        "    def __init__(self, beta: float, debug: float=None) -> None:\n",
        "        \"\"\"Constructor for Graph Environment\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        beta : float\n",
        "            Percentage of edges to rewire/update, real number between 1 and 100\n",
        "        debug : float, optional\n",
        "            Whether to print debug information, by default None\n",
        "        \"\"\"\n",
        "        assert beta >= 0 and beta <= 100, \"Beta must be between 0 and 100\"\n",
        "        self.beta = beta\n",
        "        self.debug = debug\n",
        "        self.training = None\n",
        "        self.eps = 1e-8\n",
        "\n",
        "        self.device = torch.device(\n",
        "            'cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        # List of possible actions, N+M, where N is ADD actions and M is REMOVE actions\n",
        "        self.possible_actions = None\n",
        "        # Length of the list of possible actions to add, used to distinguish\n",
        "        # between ADD and REMOVE actions in the list of possible actions\n",
        "        # when applying the action\n",
        "        self.len_add_actions = 0\n",
        "\n",
        "        # Graph State\n",
        "        self.graph = None\n",
        "        self.graph_copy = None\n",
        "        self.data_pyg = None\n",
        "        self.n_connected_components = None\n",
        "\n",
        "        # Community Algorithms\n",
        "        self.deception = None\n",
        "        self.detection = None\n",
        "        self.nmi = NormalizedMutualInformation()\n",
        "        # Community to hide\n",
        "        self.community_target = None\n",
        "        # Community Structure before the action\n",
        "        self.community_structure_old = None\n",
        "        # Community Structure after the action\n",
        "        self.community_structure_new = None\n",
        "\n",
        "        # Reward\n",
        "        self.rewards = None\n",
        "        self.old_rewards = 0\n",
        "\n",
        "        # Edge budget, i.e. the number of edges to rewire/update\n",
        "        self.edge_budget = None\n",
        "        self.used_edge_budget = None\n",
        "        self.exhausted_budget = None\n",
        "\n",
        "    @staticmethod\n",
        "    def get_possible_actions(\n",
        "        graph: nx.Graph,\n",
        "        community: List[int])->List[Tuple[int, int]]:\n",
        "        \"\"\"Returns the possible actions that can be applied to the graph.\n",
        "        An action is a tuple of two nodes, where the first node is the source\n",
        "        node and the second node is the destination node.\n",
        "        The action can be:\n",
        "            - add an edge between the two nodes, iff one belongs to the\n",
        "                community and the other does not.\n",
        "            - remove an edge between the two nodes, iff both belong to the\n",
        "                community.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        graph : nx.Graph\n",
        "            Graph where the actions will be applied\n",
        "        community : List[int]\n",
        "            Community to hide\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        List[Tuple[int, int]]\n",
        "            List of possible actions\n",
        "        \"\"\"\n",
        "        possible_actions = {\"ADD\": [], \"REMOVE\": []}\n",
        "        # Helper functions to check if a node is in/out-side the community\n",
        "        def in_community(node):\n",
        "            return node in community\n",
        "        def out_community(node):\n",
        "            return node not in community\n",
        "\n",
        "        for u in graph.nodes():\n",
        "            for v in graph.nodes():\n",
        "                if u == v:\n",
        "                    continue\n",
        "                # We can remove an edge iff both nodes are in the community\n",
        "                if in_community(u) and in_community(v):\n",
        "                    if graph.has_edge(u,v):\n",
        "                        if (v, u) not in possible_actions[\"REMOVE\"]:\n",
        "                            possible_actions[\"REMOVE\"].append((u,v))\n",
        "\n",
        "                # We can add an edge iff one node is in the community and the other is not\n",
        "                elif (in_community(u) and out_community(v)) \\\n",
        "                    or (out_community(u) and in_community(v)):\n",
        "                    # Check if there is already an edge between the two nodes\n",
        "                    if not graph.has_edge(u,v):\n",
        "                        if (v, u) not in possible_actions[\"ADD\"]:\n",
        "                            possible_actions[\"ADD\"].append((u,v))\n",
        "\n",
        "        return possible_actions\n",
        "\n",
        "    @staticmethod\n",
        "    def get_edge_budget(graph: nx.Graph, beta:float) -> int:\n",
        "        \"\"\"Computes the edge budget for each graph\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        graph : nx.Graph\n",
        "            NetworkX Graph objects, i.e. graph to compute the edge\n",
        "            budget for\n",
        "        beta : float\n",
        "            Percentage of edges to rewire/update\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        int\n",
        "            Edge budgets of the graph\n",
        "        \"\"\"\n",
        "        return int(math.ceil((graph.number_of_edges() * beta / 100)))\n",
        "\n",
        "    @staticmethod\n",
        "    def get_reward(\n",
        "        deception_score: float,\n",
        "        nmi_score: float,\n",
        "        weight: float=HyperParams.WEIGHT.value) -> float:\n",
        "        \"\"\"\n",
        "        Computes the reward for the agent\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        deception_score : float\n",
        "            Deception score\n",
        "        nmi_score : float\n",
        "            Normalized Mutual Information score\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        reward : float\n",
        "            Reward\n",
        "        \"\"\"\n",
        "        reward = weight * deception_score + (1 - weight) * nmi_score\n",
        "        return reward\n",
        "\n",
        "\n",
        "    def setup(\n",
        "        self,\n",
        "        graph: nx.Graph,\n",
        "        community: List[int],\n",
        "        community_detection_algorithm: str = DetectionAlgorithms.LOUV.value,\n",
        "        training: bool = False) -> None:\n",
        "        \"\"\"Setup function for the environment\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        graph : nx.Graph\n",
        "            NetworkX Graph object\n",
        "        community : List[int]\n",
        "            Community to hide\n",
        "        community_detection_algorithm : str, optional\n",
        "            Name of the community detection algorithm to use, by default `louv`\n",
        "        training : bool, optional\n",
        "            Whether the environment is used for training, by default False\n",
        "        \"\"\"\n",
        "        self.graph = graph\n",
        "        self.graph_copy = graph.copy()\n",
        "        self.community_target = community\n",
        "        self.training = training\n",
        "        self.rewards = 0.0\n",
        "\n",
        "        # Get the Number of connected components\n",
        "        self.n_connected_components = nx.number_connected_components(\n",
        "            self.graph)\n",
        "\n",
        "        self.detection = DetectionAlgorithm(community_detection_algorithm)\n",
        "        self.deception = DeceptionScore(self.community_target)\n",
        "\n",
        "        # Compute the community structure of the graph, before the action,\n",
        "        # i.e. before the deception\n",
        "        self.community_structure_old = self.detection.compute_community(\n",
        "            self.graph)\n",
        "\n",
        "        # Compute the edge budget for the graph\n",
        "        self.edge_budget = self.get_edge_budget(self.graph, self.beta)\n",
        "        self.used_edge_budget = 0\n",
        "        self.exhausted_budget = False\n",
        "\n",
        "        # Compute the set of possible actions\n",
        "        self.possible_actions = self.get_possible_actions(\n",
        "            self.graph, self.community_target)\n",
        "        # Length of the list of possible actions to add\n",
        "        self.len_add_actions = len(self.possible_actions[\"ADD\"])\n",
        "\n",
        "\n",
        "    def reset(self) -> Data:\n",
        "        \"\"\"Reset the environment\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        adj_matrix : torch.Tensor\n",
        "            Adjacency matrix of the graph\n",
        "        \"\"\"\n",
        "        self.used_edge_budget = 0\n",
        "        self.exhausted_budget = False\n",
        "        self.graph = self.graph_copy.copy()\n",
        "        self.possible_actions = self.get_possible_actions(self.graph, self.community_target)\n",
        "\n",
        "        # Return a PyG Data object\n",
        "        self.data_pyg = from_networkx(self.graph)\n",
        "        # self.data_pyg = self.remove_duplicated_edges(self.data_pyg)\n",
        "        # print(\"Edges:\", self.data_pyg.edge_index.shape)\n",
        "        # self.data_pyg = self.delete_repeat_edges(from_networkx(self.graph))\n",
        "\n",
        "        # Initialize the node features\n",
        "        self.data_pyg.x = torch.randn([self.data_pyg.num_nodes, HyperParams.G_IN_SIZE.value])\n",
        "        # Initialize the batch\n",
        "        self.data_pyg.batch = torch.zeros(self.data_pyg.num_nodes).long()\n",
        "        return self.data_pyg.to(self.device)\n",
        "\n",
        "    def apply_action(self, actions: np.array)->int:\n",
        "        \"\"\"Applies the action to the graph, if there is an edge between the two\n",
        "        nodes, it removes it, otherwise it adds it\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        actions : np.array\n",
        "            List of possible actions, where each element is a real number\n",
        "            between 0 and 1\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        budget_consumed : int\n",
        "            Amount of budget consumed\n",
        "        \"\"\"\n",
        "        # Get the index of the action to apply\n",
        "        index = np.argmax(actions)\n",
        "        #° The number of possible actions is:\n",
        "        #°      len(self.possible_actions[\"ADD\"]) + len(self.possible_actions[\"REMOVE\"])\n",
        "        #° So, if the index is less than the number of possible actions to add,\n",
        "        #° the action to apply is an action to add, otherwise it is an action to remove\n",
        "        if index < self.len_add_actions:\n",
        "            action = self.possible_actions[\"ADD\"][index]\n",
        "            # If the action is (-1,-1) it means that the action has already been\n",
        "            # applied, so we do not need to apply it again\n",
        "            if action == (-1,-1): return 0\n",
        "            # Apply the action\n",
        "            self.graph.add_edge(*action, weight=1)\n",
        "            # Replace the added edge with (-1,-1) in the possible actions, in this way\n",
        "            # we can keep track of the used actions, and we can avoid to add the same\n",
        "            # edge multiple times\n",
        "            self.possible_actions[\"ADD\"][index] = (-1, -1)\n",
        "            return 1\n",
        "        else:\n",
        "            action = self.possible_actions[\"REMOVE\"][index - self.len_add_actions]\n",
        "            # If the action is (-1,-1) it means that the action has already been\n",
        "            # applied, so we do not need to apply it again\n",
        "            if action == (-1, -1): return 0\n",
        "            # Apply the action\n",
        "            self.graph.remove_edge(*action)\n",
        "            # Replace the removed edge with (-1,-1) in the possible actions,\n",
        "            # in order to keep the same length, and to avoid to remove the same\n",
        "            # edge multiple times\n",
        "            self.possible_actions[\"REMOVE\"][index - self.len_add_actions] = (-1, -1)\n",
        "            return 1\n",
        "\n",
        "    def step(self, actions: np.array) -> Tuple[Data, float]:\n",
        "        \"\"\"Step function for the environment\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        actions : np.array\n",
        "            Actions to take on the graph, which is a list longer as the number\n",
        "            of possible actions, where each element is a real number between\n",
        "            0 and 1.\n",
        "        Returns\n",
        "        -------\n",
        "        self.graph, self.rewards: Tuple[torch.Tensor, float]\n",
        "            Tuple containing the new graph and the reward\n",
        "        \"\"\"\n",
        "        # Compute the remaining budget\n",
        "        remaining_budget = self.edge_budget - self.used_edge_budget\n",
        "\n",
        "        if remaining_budget <= 0:\n",
        "            self.exhausted_budget = True\n",
        "            return self.graph, self.rewards\n",
        "\n",
        "        # Take action\n",
        "        budget_consumed = self.apply_action(actions)\n",
        "        # Decrease the remaining budget\n",
        "        updated_budget = remaining_budget - budget_consumed\n",
        "\n",
        "        # Compute the new Community Structure\n",
        "        self.community_structure_new = self.detection.compute_community(\n",
        "            self.graph)\n",
        "\n",
        "        # Now we have the old and the new community structure, we can compute\n",
        "        # the NMI score\n",
        "        nmi = self.nmi.compute_nmi(\n",
        "            self.community_structure_old, self.community_structure_new)\n",
        "        # Compute new deception score\n",
        "        deception_score = self.deception.compute_deception_score(\n",
        "            self.community_structure_new, self.n_connected_components)\n",
        "\n",
        "        if self.debug:\n",
        "            print(\"Community Structure Old:\", self.community_structure_new)\n",
        "            print(\"Deception Score:\", deception_score)\n",
        "            print(\"NMI Score:\", nmi)\n",
        "\n",
        "\n",
        "        # Compute the reward, using the deception score and the NMI score\n",
        "        reward = self.get_reward(deception_score, nmi)\n",
        "        #TEST Subtract the old reward from the new reward\n",
        "        # reward -= self.old_rewards\n",
        "        if abs(reward) < self.eps:\n",
        "            reward = 0\n",
        "        self.rewards = reward\n",
        "        # self.old_rewards = reward\n",
        "\n",
        "        # Update the used edge budget\n",
        "        self.used_edge_budget += (remaining_budget - updated_budget)\n",
        "\n",
        "        # Return a PyG Data object\n",
        "        data = from_networkx(self.graph)\n",
        "        # Assign the node features and the batch of the old graph to the new graph\n",
        "        data.x = self.data_pyg.x\n",
        "        data.batch = self.data_pyg.batch\n",
        "        self.data_pyg = data\n",
        "        return self.data_pyg.to(self.device), self.rewards\n",
        "\n",
        "    def plot_graph(self) -> None:\n",
        "        \"\"\"Plot the graph using matplotlib\"\"\"\n",
        "        import matplotlib.pyplot as plt\n",
        "        nx.draw(self.graph, with_labels=True)\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "iQDrQKvy4qN2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "vAdiThYJ4qN3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "1JxHpEqT4qN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class GraphEncoder(nn.Module):\n",
        "    def __init__(self,in_feature, hidden_feature, out_feature):\n",
        "        super(GraphEncoder, self).__init__()\n",
        "        self.in_feature = in_feature\n",
        "        self.hidden_feature = hidden_feature\n",
        "        self.out_feature = out_feature\n",
        "\n",
        "        #Torch Geometric GCNConv\n",
        "        self.pyg_conv = GCNConv(in_feature, hidden_feature)\n",
        "        self.linear1 = nn.Linear(hidden_feature,out_feature)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.relu = torch.relu\n",
        "\n",
        "        # TEST Use GATConv\n",
        "        self.in_head = self.hidden_feature\n",
        "        self.conv1 = GATConv(\n",
        "            self.in_feature,\n",
        "            self.hidden_feature,\n",
        "            heads=self.in_head,\n",
        "            dropout=0.6)\n",
        "        self.conv2 = GATConv(\n",
        "            self.hidden_feature * self.in_head,\n",
        "            self.out_feature,\n",
        "            concat=False,\n",
        "            heads=self.out_feature,\n",
        "            dropout=0.6)\n",
        "\n",
        "    #NOTE Torch Geometric MessagePassing, it takes as input the edge list\n",
        "    def forward(self, graph: Data)-> torch.Tensor:\n",
        "        x, edge_index, batch = graph.x, graph.edge_index, graph.batch\n",
        "        x = self.pyg_conv(x, edge_index)\n",
        "        x = self.relu(x)\n",
        "        embedding = global_mean_pool(x, batch)\n",
        "        embedding = self.linear1(embedding)\n",
        "        embedding = self.tanh(embedding)\n",
        "        return embedding\n",
        "\n",
        "    # TEST Use GATConv\n",
        "    # def forward(self, graph: Data)-> torch.Tensor:\n",
        "    #     x, edge_index, batch = graph.x, graph.edge_index, graph.batch\n",
        "    #     # Dropout before the GAT layer is used to avoid overfitting in small datasets like Cora.\n",
        "    #     # One can skip them if the dataset is sufficiently large.\n",
        "    #     x = F.dropout(x, p=0.6, training=self.training)\n",
        "    #     x = self.conv1(x, edge_index)\n",
        "    #     x = F.elu(x)\n",
        "    #     x = F.dropout(x, p=0.6, training=self.training)\n",
        "    #     x = self.conv2(x, edge_index)\n",
        "    #     x = self.relu(x)\n",
        "    #     embedding = global_mean_pool(x, batch)\n",
        "    #     embedding = self.linear1(embedding)\n",
        "    #     embedding = self.tanh(embedding)\n",
        "    #     return embedding"
      ],
      "metadata": {
        "trusted": true,
        "id": "O5TwC47m4qN3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A2C"
      ],
      "metadata": {
        "id": "1S7ubGoY4qN4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Actor"
      ],
      "metadata": {
        "id": "SfScUOtU4qN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorNetwork(nn.Module):\n",
        "    \"\"\"Actor Network\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            g_in_size,\n",
        "            g_hidden_size,\n",
        "            g_embedding_size,\n",
        "            hidden_size,\n",
        "            nb_actions,\n",
        "            chkpt_dir=FilePaths.LOG_DIR.value):\n",
        "        super(ActorNetwork, self).__init__()\n",
        "\n",
        "        self.checkpoint_file = os.path.join(chkpt_dir, 'actor_torch_rl')\n",
        "        self.graph_encoder = GraphEncoder(\n",
        "            in_feature=g_in_size,\n",
        "            hidden_feature=g_hidden_size,\n",
        "            out_feature=g_embedding_size)\n",
        "        self.linear1 = nn.Linear(g_embedding_size, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, nb_actions)\n",
        "        self.nb_actions = nb_actions\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        #TODO try with a Softmax\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        self.device = torch.device(\n",
        "            'cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, state: Data):\n",
        "        g = self.graph_encoder(state)\n",
        "        actions = self.relu(self.linear1(g))\n",
        "        # actions = self.tanh(self.linear2(actions))\n",
        "        actions = self.softmax(self.linear2(actions))\n",
        "\n",
        "        return actions\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        torch.save(self.state_dict(), self.checkpoint_file)\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        self.load_state_dict(torch.load(self.checkpoint_file))"
      ],
      "metadata": {
        "trusted": true,
        "id": "HV4Ef1504qN4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Critic"
      ],
      "metadata": {
        "id": "4L57IBdh4qN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CriticNetwork(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        g_in_size,\n",
        "        g_hidden_size,\n",
        "        g_embedding_size,\n",
        "        chkpt_dir=FilePaths.LOG_DIR.value):\n",
        "        super(CriticNetwork, self).__init__()\n",
        "\n",
        "        self.checkpoint_file = os.path.join(chkpt_dir, 'critic_torch_rl')\n",
        "        self.graph_encoder_critic = GraphEncoder(\n",
        "            g_in_size, g_hidden_size, g_embedding_size)\n",
        "        self.linear1 = nn.Linear(g_embedding_size, 1)\n",
        "        self.tanh = nn.Tanh()\n",
        "        # self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
        "\n",
        "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, state: Data):\n",
        "        g = self.graph_encoder_critic(state)\n",
        "        value = self.tanh(self.linear1(g))\n",
        "        return value\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        torch.save(self.state_dict(), self.checkpoint_file)"
      ],
      "metadata": {
        "trusted": true,
        "id": "jRxY8xcL4qN4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Memory"
      ],
      "metadata": {
        "id": "tTRmOASJ4qN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Memory:\n",
        "    \"\"\"Memory for storing the agent's experience tuples\"\"\"\n",
        "    def __init__(self):\n",
        "        self.actions = []\n",
        "        self.states = []\n",
        "        self.logprobs = []\n",
        "        self.rewards = []\n",
        "        self.is_terminals = []\n",
        "\n",
        "    def clear_memory(self):\n",
        "        \"\"\"Clear the memory\"\"\"\n",
        "        del self.actions[:]\n",
        "        del self.states[:]\n",
        "        del self.logprobs[:]\n",
        "        del self.rewards[:]\n",
        "        del self.is_terminals[:]"
      ],
      "metadata": {
        "trusted": true,
        "id": "amzQZbYG4qN5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Network"
      ],
      "metadata": {
        "id": "PJxe7FGi4qN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    \"\"\"ActorCritic Network\"\"\"\n",
        "    def __init__(self, state_dim, action_dim, action_std):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        # action mean range -1 to 1\n",
        "        actor_cfg = {\n",
        "            'g_in_size': state_dim,\n",
        "            'g_hidden_size': HyperParams.G_HIDDEN_SIZE.value,\n",
        "            'g_embedding_size': HyperParams.G_EMBEDDING_SIZE.value,\n",
        "            'hidden_size': HyperParams.HIDDEN_SIZE.value,\n",
        "            'nb_actions': action_dim,\n",
        "        }\n",
        "        critic_cfg = {\n",
        "            'g_in_size': state_dim,\n",
        "            'g_hidden_size': HyperParams.G_HIDDEN_SIZE.value,\n",
        "            'g_embedding_size': HyperParams.G_EMBEDDING_SIZE.value,\n",
        "        }\n",
        "        self.actor = ActorNetwork(**actor_cfg)\n",
        "        self.critic = CriticNetwork(**critic_cfg)\n",
        "\n",
        "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "        self.action_var = torch.full(\n",
        "            (action_dim,), action_std*action_std).to(self.device)\n",
        "\n",
        "    def forward(self):\n",
        "        \"\"\"Forward pass\"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def act(\n",
        "        self,\n",
        "        state: Data,\n",
        "        memory: Memory)-> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Compute the action to take given the current state\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        state : torch.Tensor\n",
        "            Current state\n",
        "        memory : Memory\n",
        "            Memory object\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Tuple[torch.Tensor, torch.Tensor]\n",
        "            The action to take and the log probability of the action\n",
        "        \"\"\"\n",
        "        action_mean = self.actor(state)\n",
        "\n",
        "        cov_mat = torch.diag(self.action_var).to(self.device)\n",
        "        dist = MultivariateNormal(action_mean, cov_mat)\n",
        "        action = dist.sample()\n",
        "        action_logprob = dist.log_prob(action)\n",
        "\n",
        "        memory.states.append(state)\n",
        "        memory.actions.append(action)\n",
        "        memory.logprobs.append(action_logprob)\n",
        "\n",
        "        return action.detach()\n",
        "        #return action.detach(), action_logprob.detach()\n",
        "\n",
        "    def evaluate(self, state: torch.Tensor, action)-> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Evaluate the current state and action\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        state : _type_\n",
        "            _description_\n",
        "        action : _type_\n",
        "            _description_\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
        "            The log probability of the action, the state value, and the entropy\n",
        "        \"\"\"\n",
        "        action_mean = self.actor(state)\n",
        "\n",
        "        action_var = self.action_var.expand_as(action_mean)\n",
        "        cov_mat = torch.diag_embed(action_var).to(self.device)\n",
        "\n",
        "        dist = MultivariateNormal(action_mean, cov_mat)\n",
        "\n",
        "        action_logprobs = dist.log_prob(action)\n",
        "        dist_entropy = dist.entropy()\n",
        "        state_value = self.critic(state)\n",
        "\n",
        "        return action_logprobs, state_value, dist_entropy"
      ],
      "metadata": {
        "trusted": true,
        "id": "uqcYwWsV4qN5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agent"
      ],
      "metadata": {
        "id": "6mrEeBM-4qN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Agent:\n",
        "    def __init__(self, state_dim, action_dim, action_std, lr, gamma, K_epochs, eps_clip):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.action_std = action_std\n",
        "        self.lr = lr\n",
        "        # self.betas = betas\n",
        "        self.gamma = gamma\n",
        "        self.eps_clip = eps_clip\n",
        "        self.K_epochs = K_epochs\n",
        "\n",
        "        self.memory = Memory()\n",
        "\n",
        "        self.device = torch.device(\n",
        "            'cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        self.policy = ActorCritic(state_dim, action_dim, action_std).to(self.device)\n",
        "        # self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, betas=betas)\n",
        "        self.optimizer = torch.optim.Adam(\n",
        "            filter(lambda p: p.requires_grad, self.policy.parameters()), lr=lr)#, betas=betas)\n",
        "\n",
        "        self.policy_old = ActorCritic(\n",
        "            state_dim, action_dim, action_std).to(self.device)\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        self.MseLoss = nn.MSELoss()\n",
        "\n",
        "    def select_action(self, state: Data, memory: Memory,) -> list:\n",
        "        \"\"\"\n",
        "        Select an action given the current state\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        state : _type_\n",
        "            state\n",
        "        memory : Memory\n",
        "            Memory object\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        action: torch.Tensor\n",
        "            Action to take\n",
        "        \"\"\"\n",
        "        #  # state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
        "        # ! OLD\n",
        "        # return self.policy_old.act(state, memory).cpu().data.numpy().flatten()\n",
        "\n",
        "        # ! NEW\n",
        "        with torch.no_grad():\n",
        "            # state = torch.FloatTensor(state).to(device)\n",
        "            action = self.policy_old.act(state, memory)\n",
        "        return action.cpu().data.numpy().flatten()\n",
        "        #return action.tolist()\n",
        "\n",
        "    def update(self, memory: Memory):\n",
        "        \"\"\"\n",
        "        Update the policy\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        memory : Memory\n",
        "            Memory object\n",
        "        \"\"\"\n",
        "        rewards = []\n",
        "        discounted_reward = 0\n",
        "        # Compute the Monte Carlo estimate of the rewards for each time step in\n",
        "        # the episode. This involves iterating over the rewards in reverse order\n",
        "        # and computing the discounted sum of rewards from each time step to the\n",
        "        # end of the episode.\n",
        "        # The resulting rewards are then normalized by subtracting the mean\n",
        "        # and dividing by the standard deviation.\n",
        "        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n",
        "            if is_terminal:\n",
        "                discounted_reward = 0\n",
        "\n",
        "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
        "            rewards.insert(0, discounted_reward)\n",
        "\n",
        "        # Normalizing the rewards\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
        "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
        "\n",
        "        # Prepares the data for training the policy network.\n",
        "        # The memory object contains lists of states, actions, log probabilities,\n",
        "        # and rewards for each time step in the episode.\n",
        "\n",
        "        # Each state is a PyG Data object\n",
        "        old_states = Batch.from_data_list(memory.states).to(self.device)\n",
        "        old_actions = torch.squeeze(torch.stack(memory.actions, dim=1)).detach().to(self.device)\n",
        "        old_logprobs = torch.squeeze(torch.stack(memory.logprobs, dim=1)).detach().to(self.device)\n",
        "\n",
        "        # Optimize policy for K epochs\n",
        "        for i in range(self.K_epochs):\n",
        "\n",
        "            # The loss function is computed using the ratio of the probabilities\n",
        "            # of the actions under the new and old policies, multiplied by the\n",
        "            # advantage of taking the action. The advantage is the difference\n",
        "            # between the discounted sum of rewards and the estimated value of\n",
        "            # the state under the current policy. The loss is also augmented\n",
        "            # with a term that encourages the policy to explore different actions.\n",
        "\n",
        "            # Evaluating old actions and values\n",
        "            logprobs, state_values, dist_entropy = self.policy.evaluate(\n",
        "                old_states, old_actions)\n",
        "\n",
        "            # match state_values tensor dimensions with rewards tensor\n",
        "            state_values = torch.squeeze(state_values)\n",
        "\n",
        "            # Finding the ratio (pi_theta / pi_theta__old)\n",
        "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
        "            # Finding Surrogate Loss\n",
        "            advantages = rewards - state_values.detach()\n",
        "\n",
        "            surr1 = ratios * advantages\n",
        "            surr2 = torch.clamp(ratios, 1-self.eps_clip,\n",
        "                                1+self.eps_clip) * advantages\n",
        "\n",
        "            # Final loss: first term is Actor Loss, second term is Critic Loss\n",
        "            act_loss = -torch.min(surr1, surr2)\n",
        "            crt_loss = self.MseLoss(state_values, rewards) * 0.5\n",
        "            ent_loss = dist_entropy * 0.01\n",
        "            loss = act_loss + crt_loss - ent_loss # Want to maximize\n",
        "\n",
        "            if (i+1) % 5 == 0 or i == 0:\n",
        "                print('* Epoches {} \\t loss: {} \\t '.format(i+1, loss.mean()))\n",
        "\n",
        "            # take gradient step\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        # Copy new weights into old policy:\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "        # Clear memory\n",
        "        memory.clear_memory()\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        env: GraphEnvironment,\n",
        "        memory: Memory,\n",
        "        max_episodes: int = HyperParams.MAX_EPISODES.value,\n",
        "        max_timesteps: int = HyperParams.MAX_TIMESTEPS.value,\n",
        "        update_timesteps: int = HyperParams.UPDATE_TIMESTEP.value,\n",
        "        log_interval: int = HyperParams.LOG_INTERVAL.value,\n",
        "        solved_reward: float = HyperParams.SOLVED_REWARD.value,\n",
        "        save_model: int = HyperParams.SAVE_MODEL.value,\n",
        "        log_dir: str = FilePaths.LOG_DIR.value,\n",
        "        env_name: str = \"Default\") -> None:\n",
        "        \"\"\"\n",
        "        Function to train the agent\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        max_episodes : int, optional\n",
        "            Number of episodes,by default HyperParams.MAX_EPISODES.value\n",
        "        max_timesteps : int, optional\n",
        "            Number of timesteps, by default HyperParams.MAX_TIMESTEPS.value\n",
        "        update_timesteps : int, optional\n",
        "            Number of timesteps to update the policy, by default HyperParams.UPDATE_TIMESTEP.value\n",
        "        log_interval : int, optional\n",
        "            Interval for logging, by default HyperParams.LOG_INTERVAL.value\n",
        "        solved_reward : float, optional\n",
        "            Stop training if the reward is greater than this value, by default HyperParams.SOLVED_REWARD.value\n",
        "        save_model : int, optional\n",
        "            Each save_model episodes save the model, by default HyperParams.SAVE_MODEL.value\n",
        "        log_dir : str, optional\n",
        "            Directory for logging, by default FilePaths.LOG_DIR.value\n",
        "        env_name : str, optional\n",
        "            Environment name, by default \"Default\"\n",
        "        \"\"\"\n",
        "        # Logging Variables\n",
        "        running_reward = 0\n",
        "        avg_length = 0\n",
        "        time_step = 0\n",
        "\n",
        "        episodes_avg_rewards = []\n",
        "        episodes_length = []\n",
        "\n",
        "        # ! Comment this line if you are on Kaggle or Colab\n",
        "        log_dir = './' + log_dir\n",
        "\n",
        "        # Training loop\n",
        "        for episode in range(1, max_episodes + 1):\n",
        "            # Reset the environment at each episode, state is a PyG Data object\n",
        "            state = env.reset()\n",
        "            done = False\n",
        "            print(\"*\" * 20, \"Start Episode\", episode, \"*\" * 20)\n",
        "\n",
        "            avg_episode_reward = 0\n",
        "            avg_episode_timesteps = 0\n",
        "            for t in range(max_timesteps):\n",
        "                # If the budget for the graph rewiring is exhausted, stop the episode\n",
        "                if env.used_edge_budget == env.edge_budget - 1:\n",
        "                    print(\"*\", \"-\" * 19, \"Budget exhausted\", \"-\" * 19)\n",
        "                    done = True\n",
        "                time_step += 1\n",
        "                # ° Running policy_old, return a distribution over the actions\n",
        "                actions = self.select_action(state, memory)\n",
        "                # ° Perform the step on the environment, i.e. add or remove an edge\n",
        "                state, reward = env.step(actions)\n",
        "                # ° Saving reward and is_terminals\n",
        "                memory.rewards.append(reward)\n",
        "                memory.is_terminals.append(done)\n",
        "                # ° Update policy if its time\n",
        "                if time_step % update_timesteps == 0:\n",
        "                    print(\"*\", \"-\" * 13, \"Start training the RL agent \", \"-\" * 13)\n",
        "                    self.update(memory)\n",
        "                    memory.clear_memory()\n",
        "                    time_step = 0\n",
        "                    print(\"*\", \"-\"*14, \"End training the RL agent \", \"-\"*14)\n",
        "                # Add the reward to the running reward\n",
        "                running_reward += reward\n",
        "                # Update the average episode reward and timesteps\n",
        "                avg_episode_reward += reward\n",
        "                avg_episode_timesteps += 1\n",
        "                # Check if the episode is done\n",
        "                if done:\n",
        "                    break\n",
        "            # Show Average Episode Reward and Timesteps\n",
        "            print(\"* Average Episode Reward: \",\n",
        "                avg_episode_reward/avg_episode_timesteps)\n",
        "            print(\"* Episode Timesteps: \", avg_episode_timesteps)\n",
        "            episodes_avg_rewards.append(avg_episode_reward/avg_episode_timesteps)\n",
        "            episodes_length.append(avg_episode_timesteps)\n",
        "            avg_episode_reward = 0\n",
        "            avg_episode_timesteps = 0\n",
        "            # Update the average length of episodes\n",
        "            avg_length += t+1\n",
        "            # ° Stop training if avg_reward > solved_reward\n",
        "            if (episode % log_interval) > 10 and running_reward / avg_length > solved_reward:\n",
        "                print(\"#\"*20, \"Solved\", \"#\"*20)\n",
        "                print(\"Running reward: \", running_reward/avg_length)\n",
        "                torch.save(self.policy.state_dict(),\n",
        "                        log_dir + '{}_rl_solved.pth'.format(env_name))\n",
        "                break\n",
        "            # ° Save model\n",
        "            if episode % save_model == 0:\n",
        "                print(\"*\", \"-\"*19, \"\\tSaving Model  \", \"-\"*19)\n",
        "                torch.save(self.policy.state_dict(),\n",
        "                        log_dir + '{}_rl.pth'.format(env_name))\n",
        "                torch.save(self.policy.actor.graph_encoder.state_dict(),\n",
        "                        log_dir + '{}_rl_graph_encoder_actor.pth'.format(env_name))\n",
        "                torch.save(self.policy.critic.graph_encoder_critic.state_dict(),\n",
        "                        log_dir + '{}_rl_graph_encoder_critic.pth'.format(env_name))\n",
        "            # ° Log details\n",
        "            if episode % log_interval == 0:\n",
        "                avg_length = int(avg_length / log_interval)\n",
        "                running_reward = int((running_reward / log_interval))\n",
        "                print(\"*\", \"-\"*56)\n",
        "                print('* Episode {}\\t avg log length: {}\\t avg log reward: {:.2f}'.format(\n",
        "                    episode, avg_length, running_reward/avg_length))\n",
        "                print(\"*\", \"-\"*56)\n",
        "                running_reward = 0\n",
        "                avg_length = 0\n",
        "\n",
        "            if env.debug:\n",
        "                env.plot_graph()\n",
        "            print(\"*\"*57, \"\\n\")\n",
        "\n",
        "        hyperparams_dict = {\n",
        "            \"max_episodes\": max_episodes,\n",
        "            \"max_timesteps\": max_timesteps,\n",
        "            \"update_timesteps\": update_timesteps,\n",
        "            \"log_interval\": log_interval,\n",
        "            \"solved_reward\": solved_reward,\n",
        "            \"save_model\": save_model,\n",
        "            \"state_dim\": self.state_dim,\n",
        "            \"action_dim\": self.action_dim,\n",
        "            \"action_std\": self.action_std,\n",
        "            \"lr\": self.lr,\n",
        "            \"gamma\": self.gamma,\n",
        "            \"K_epochs\": self.K_epochs,\n",
        "            \"eps_clip\": self.eps_clip,\n",
        "        }\n",
        "        # Save lists and hyperparameters in a json file\n",
        "        print(\"*\", \"-\"*18, \"Saving results\", \"-\"*18)\n",
        "        Utils.write_results_to_json(\n",
        "            episodes_avg_rewards, episodes_length, hyperparams_dict, env_name)\n",
        "\n",
        "        print(\"*\", \"-\"*18, \"Plotting results\", \"-\"*18)\n",
        "        # Plot the average reward per episode\n",
        "        Utils.plot_avg_reward(episodes_avg_rewards, episodes_length, env_name)"
      ],
      "metadata": {
        "trusted": true,
        "id": "V4rI4Jrv4qN5"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "oAXtKihw4qN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execution"
      ],
      "metadata": {
        "id": "Pnf9azpI4qN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"*\"*20, \"Setup Information\", \"*\"*20)\n",
        "\n",
        "# ° ------ Graph Setup ------ ° #\n",
        "# ! Graph path (change the following line to change the graph)\n",
        "graph_path = FilePaths.KAR.value\n",
        "# Set the environment name as the graph name\n",
        "env_name = graph_path.split(\"/\")[-1].split(\".\")[0]\n",
        "# Load the graph from the dataset folder\n",
        "graph = Utils.import_mtx_graph(graph_path)\n",
        "# Print the number of nodes and edges\n",
        "print(\"* Graph Name:\", env_name)\n",
        "print(\"*\", graph)\n",
        "\n",
        "# ° --- Environment Setup --- ° #\n",
        "# ! Define the detection algorithm to use (change the following line to change the algorithm)\n",
        "detection_alg = DetectionAlgorithms.WALK.value\n",
        "# Apply the community detection algorithm on the graph\n",
        "dct = DetectionAlgorithm(detection_alg)\n",
        "community_structure = dct.compute_community(graph)\n",
        "# Choose one of the communities found by the algorithm, for now we choose the first one\n",
        "community_target = community_structure[0]\n",
        "print(\"* Community Detection Algorithm:\", detection_alg)\n",
        "print(\"* Community Target:\", community_target)\n",
        "# Define beta, i.e. the percentage of edges to add/remove\n",
        "beta = HyperParams.BETA.value\n",
        "# Define the environment\n",
        "env = GraphEnvironment(beta=beta, debug=False)\n",
        "# Setup the environment\n",
        "env.setup(\n",
        "    graph=graph,\n",
        "    community=community_target,\n",
        "    training=True,\n",
        "    community_detection_algorithm=detection_alg)\n",
        "# Get list of possible actions which can be performed on the graph by the agent\n",
        "possible_actions = env.get_possible_actions(graph, community_target)\n",
        "n_actions = len(possible_actions[\"ADD\"]) + len(possible_actions[\"REMOVE\"])\n",
        "print(\"* Number of possible actions:\", n_actions)\n",
        "\n",
        "# ° ------ Agent Setup ------ ° #\n",
        "# Dimensions of the state\n",
        "state_dim = HyperParams.G_IN_SIZE.value\n",
        "# Number of possible actions\n",
        "action_dim = n_actions\n",
        "# Standard deviation for the action\n",
        "action_std = HyperParams.ACTION_STD.value\n",
        "# Learning rate\n",
        "lr = HyperParams.LR.value\n",
        "# Gamma parameter\n",
        "gamma = HyperParams.GAMMA.value\n",
        "# Number of epochs when updating the policy\n",
        "k_epochs = HyperParams.K_EPOCHS.value\n",
        "# Value for clipping the loss function\n",
        "eps_clip = HyperParams.EPS_CLIP.value\n",
        "# Define the agent\n",
        "agent = Agent(\n",
        "    state_dim=state_dim,\n",
        "    action_dim=action_dim,\n",
        "    action_std=action_std,\n",
        "    lr=lr,\n",
        "    gamma=gamma,\n",
        "    K_epochs=k_epochs,\n",
        "    eps_clip=eps_clip)\n",
        "# Define Memory\n",
        "memory = Memory()\n",
        "# Print Hyperparameters\n",
        "print(\"*\", \"-\"*18, \"Hyperparameters\", \"-\"*18)\n",
        "print(\"* State dimension: \", state_dim)\n",
        "print(\"* Action dimension: \", action_dim)\n",
        "print(\"* Action standard deviation: \", action_std)\n",
        "print(\"* Learning rate: \", lr)\n",
        "print(\"* Gamma parameter: \", gamma)\n",
        "print(\"* Number of epochs when updating the policy: \", k_epochs)\n",
        "print(\"* Value for clipping the loss function: \", eps_clip)\n",
        "print(\"*\", \"-\"*53)\n",
        "\n",
        "# Set random seed\n",
        "# random_seed = HyperParams.RANDOM_SEED.value\n",
        "# if random_seed:\n",
        "#     print(\"* Random Seed: {}\".format(random_seed))\n",
        "#     torch.manual_seed(random_seed)\n",
        "#     env.seed(random_seed)\n",
        "#     np.random.seed(random_seed)\n",
        "print(\"*\"*20, \"End Information\", \"*\"*20, \"\\n\")\n",
        "\n",
        "# ° ------ Model Training ------ ° #\n",
        "# Set the maximum number of steps per episode to the double of the edge budget\n",
        "max_timesteps = env.edge_budget*2\n",
        "# Set the update timestep to 10 times then edge budget\n",
        "update_timesteps = env.edge_budget*10\n",
        "# Start training\n",
        "agent.train(\n",
        "    env=env,\n",
        "    memory=memory,\n",
        "    max_timesteps=max_timesteps,\n",
        "    update_timesteps=update_timesteps,\n",
        "    env_name=env_name)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Lvmw7taY4qN7",
        "outputId": "9446f2ab-8097-4629-fca2-ab4e79549689"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "******************** Setup Information ********************\n",
            "* Graph Name: words\n",
            "* Graph with 112 nodes and 425 edges\n",
            "* Community Detection Algorithm: walktrap\n",
            "* Community Target: [0, 1, 2, 3, 8, 12, 18, 41, 44, 45, 46, 50, 53, 61, 73, 85, 93, 102]\n",
            "* Number of possible actions: 1638\n",
            "* ------------------ Hyperparameters ------------------\n",
            "* State dimension:  50\n",
            "* Action dimension:  1638\n",
            "* Action standard deviation:  0.5\n",
            "* Learning rate:  0.0003\n",
            "* Gamma parameter:  0.99\n",
            "* Number of epochs when updating the policy:  20\n",
            "* Value for clipping the loss function:  0.2\n",
            "* -----------------------------------------------------\n",
            "******************** End Information ******************** \n",
            "\n",
            "******************** Start Episode 1 ********************\n",
            "* ------------------- Budget exhausted -------------------\n",
            "* Average Episode Reward:  0.5756552672007688\n",
            "* Episode Timesteps:  132\n",
            "********************************************************* \n",
            "\n",
            "******************** Start Episode 2 ********************\n",
            "* ------------------- Budget exhausted -------------------\n",
            "* Average Episode Reward:  0.6133742906159124\n",
            "* Episode Timesteps:  132\n",
            "********************************************************* \n",
            "\n",
            "******************** Start Episode 3 ********************\n",
            "* ------------------- Budget exhausted -------------------\n",
            "* Average Episode Reward:  0.6165942508984267\n",
            "* Episode Timesteps:  131\n",
            "********************************************************* \n",
            "\n",
            "******************** Start Episode 4 ********************\n",
            "* ------------------- Budget exhausted -------------------\n",
            "* Average Episode Reward:  0.6028342536744306\n",
            "* Episode Timesteps:  132\n",
            "********************************************************* \n",
            "\n",
            "******************** Start Episode 5 ********************\n",
            "* ------------------- Budget exhausted -------------------\n",
            "* Average Episode Reward:  0.5725361558352524\n",
            "* Episode Timesteps:  129\n",
            "********************************************************* \n",
            "\n",
            "******************** Start Episode 6 ********************\n",
            "* ------------------- Budget exhausted -------------------\n",
            "* Average Episode Reward:  0.6296492085400224\n",
            "* Episode Timesteps:  129\n",
            "********************************************************* \n",
            "\n",
            "******************** Start Episode 7 ********************\n",
            "* ------------------- Budget exhausted -------------------\n",
            "* Average Episode Reward:  0.631316002160985\n",
            "* Episode Timesteps:  133\n",
            "********************************************************* \n",
            "\n",
            "******************** Start Episode 8 ********************\n",
            "* ------------------- Budget exhausted -------------------\n",
            "* Average Episode Reward:  0.6553796835113403\n",
            "* Episode Timesteps:  133\n",
            "********************************************************* \n",
            "\n",
            "******************** Start Episode 9 ********************\n",
            "* ------------------- Budget exhausted -------------------\n",
            "* Average Episode Reward:  0.6498098690039398\n",
            "* Episode Timesteps:  131\n",
            "********************************************************* \n",
            "\n",
            "******************** Start Episode 10 ********************\n",
            "* ------------- Start training the RL agent  -------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-59a70a609ddd>\u001b[0m in \u001b[0;36m<cell line: 92>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0mupdate_timesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_budget\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m agent.train(\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mmemory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-972b29dfe041>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, env, memory, max_episodes, max_timesteps, update_timesteps, log_interval, solved_reward, save_model, log_dir, env_name)\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtime_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mupdate_timesteps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Start training the RL agent \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m                     \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                     \u001b[0mtime_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-972b29dfe041>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, memory)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;31m# Evaluating old actions and values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             logprobs, state_values, dist_entropy = self.policy.evaluate(\n\u001b[0m\u001b[1;32m    104\u001b[0m                 old_states, old_actions)\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-8e09e397c210>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mcov_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultivariateNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0maction_logprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributions/multivariate_normal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loc, covariance_matrix, precision_matrix, scale_tril, validate_args)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mevent_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscale_tril\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0;32mcontinue\u001b[0m  \u001b[0;31m# skip checking lazily-constructed args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                 \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                     raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributions/constraints.py\u001b[0m in \u001b[0;36mcheck\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    507\u001b[0m     \"\"\"\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m         \u001b[0msym_check\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msym_check\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msym_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributions/constraints.py\u001b[0m in \u001b[0;36mcheck\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msquare_check\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msquare_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.20 GiB (GPU 0; 14.75 GiB total capacity; 13.07 GiB already allocated; 770.81 MiB free; 13.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    }
  ]
}