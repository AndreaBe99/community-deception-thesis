# Test

This folder is organized as follows:

- each folder is named as `model:DATASET-ALGORITHM`, where `DATASET` and `ALGORITHM` are the names of the dataset, and of the detection algorithm used to train the model, used for the test. This folder contains:
  - the `model:DATASET` folder, which contains the model used for the test;
  - the `test_N-EPOCHs:DATASET` folder, which contains the test files of the `DATASET` dataset, tested on `N` epochs, and it contains:
    - `ALGORITHM` folder, which contains the output of the test files of the `ALGORITHM` detection algorithm;

This folder contains the output of the test files, each folder contains the files generated by the test on a specific dataset, and each of these folders contains the folders of the detection algorithms used for the test.

Below are the aggregated explanations of what we expect to see in the output files.

- **Lesson** — Plots the progress from lesson to lesson. Only interesting when performing curriculum training.
- **Cumulative Reward** — The mean cumulative episode reward over all agents. Should increase during a successful training session. The general trend in reward should consistently increase over time. Small ups and downs are to be expected. Depending on the complexity of the task, a significant increase in reward may not present itself until millions of steps into the training process.
- **Entropy** — How random the decisions of the model are. Should slowly decrease during a successful training process. If it decreases too quickly, the beta hyperparameter should be increased. This corresponds to how random the decisions of a Brain are. This should consistently decrease during training. If it decreases too soon or not at all, beta should be adjusted (when using discrete action space).
- **Episode Length** — The mean length of each episode in the environment for all agents.
- **Learning Rate** — How large a step the training algorithm takes as it searches for the optimal policy. Should decrease over time. This will decrease over time on a linear schedule.
- **Policy Loss** — The mean magnitude of policy loss function. Correlates to how much the policy (process for deciding actions) is changing. The magnitude of this should decrease during a successful training session. These values will oscillate during training. Generally they should be less than 1.0.
- **Value Estimate** — The mean value estimate for all states visited by the agent. Should increase during a successful training session. These values should increase as the cumulative reward increases. They correspond to how much future reward the agent predicts itself receiving at any given point.
- **Value Loss** — The mean loss of the value function update. Correlates to how well the model is able to predict the value of each state. This should increase while the agent is learning, and then decrease once the reward stabilizes. These values will increase as the reward increases, and then should decrease once reward becomes stable.
